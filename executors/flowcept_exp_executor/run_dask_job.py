import sys
import argparse
import os
from dataclasses import dataclass
from pathlib import Path
from time import time
import json
from datetime import datetime

from omegaconf import OmegaConf, DictConfig

from cluster_experiment_utils.cluster_utils.base_cluster_utils import BaseClusterUtils
from cluster_experiment_utils.flowcept_utils import (
    update_flowcept_settings,
    kill_dbs,
    start_redis,
    start_mongo,
    test_data_and_persist,
)

from cluster_experiment_utils.utils import run_cmd_check_output, printed_sleep, run_cmd


@dataclass
class Args:
    my_job_id: str = None
    conf: str = None  # Path
    varying_param_key: str = None


def parse_args():
    parser = argparse.ArgumentParser(description="Submit Dask Job.")

    parser.add_argument(
        "--my-job-id", metavar="i", required=True, help="job id generated by our script"
    )

    parser.add_argument("--conf", help="Yaml configuration file", required=True)

    parser.add_argument("--varying_param_key", required=True)

    if len(sys.argv) == 1:
        parser.print_help()

    ags = parser.parse_args()
    return Args(
        my_job_id=ags.my_job_id, conf=ags.conf, varying_param_key=ags.varying_param_key
    )


def start_scheduler(preload_scheduler_cmd, rep_dir, scheduler_file):
    scheduler_cmd = f"dask scheduler {preload_scheduler_cmd} --interface='ib0' --no-dashboard --no-show --scheduler-file {scheduler_file}"  # TODO: check about this interface=Ã­b0, not sure if we have this on Frontier.

    print("Starting Scheduler")
    cluster_utils = BaseClusterUtils.get_instance()
    cluster_utils.run_job(
        cmd=scheduler_cmd,
        node_count=1,
        process_count=1,
        processes_per_node=1,
        gpu_cores_per_process=0,
        stderr=f"{rep_dir}/scheduler_err.%h.%j.%t.%p",
        stdout=f"{rep_dir}/scheduler_out.%h.%j.%t.%p",
    )
    total_wait_time = 160  # in seconds
    print(
        f"Waiting up to {total_wait_time} seconds so the scheduler file can be created."
    )
    wait_time = 3
    start_waiting_time = time()
    elapsed_time = 0
    while not os.path.exists(scheduler_file) and elapsed_time < total_wait_time:
        printed_sleep(wait_time)
        elapsed_time = time() - start_waiting_time
        print(f"Still waiting for {scheduler_file}")
    if os.path.exists(scheduler_file):
        print(f"File '{scheduler_file}' found after {elapsed_time} seconds")
        print("Scheduler started!")
        return True
    else:
        print(f"File '{scheduler_file}' not found after {total_wait_time} seconds")
        return False


def start_workers_with_gpu(nnodes, n_gpus_per_node, gpu_type, rep_dir, scheduler_file):
    # From: https://docs.olcf.ornl.gov/systems/frontier_user_guide.html
    # Due to the unique architecture of Frontier compute nodes and the way
    # that Slurm currently allocates GPUs and CPU cores to job steps,
    # it is suggested that all 8 GPUs on a node are allocated to the job step
    # to ensure that optimal bindings are possible.

    if gpu_type == "amd":
        visible_device = "ROCR_VISIBLE_DEVICES"
    elif gpu_type == "nvidia":
        visible_device = "CUDA_VISIBLE_DEVICES"
    else:
        raise ValueError("Unknown gpu")

    worker_logs = os.path.join(rep_dir, "worker_logs")
    os.makedirs(worker_logs, exist_ok=True)
    cluster_utils = BaseClusterUtils.get_instance()

    for i in range(nnodes):
        for j in range(n_gpus_per_node):
            stdout = os.path.join(worker_logs, f"worker_{i}_{j}.out")
            stderr = os.path.join(worker_logs, f"worker_{i}_{j}.err")
            worker_cmd = f"""
            export {visible_device}='{j}' &&
            dask worker --nthreads 1 --nworkers 1 --interface ib0 --no-dashboard  --scheduler-file {scheduler_file} > {stdout} 2> {stderr} &
            """
            cluster_utils.run_job(worker_cmd, node_count=1, processes_per_node=1)

    print(
        f"\n\nDone starting {nnodes*n_gpus_per_node} workers. Let's just wait some time...\n\n"
    )
    printed_sleep(30)


def start_client(conf_data, varying_param_key, with_flowcept_arg):
    print("Starting the Client")
    python_client_command = conf_data.static_params.dask_user_workflow
    # TODO: what to change below?
    wf_params = conf_data["varying_params"][varying_param_key]["workflow_params"]
    # TODO: save them?

    for k, v in wf_params.items():
        python_client_command = python_client_command.replace("${" + k + "_val}", v)
    python_client_command += with_flowcept_arg
    t_c_i = time()
    run_cmd_check_output(python_client_command)

    t_c_f = time()
    printed_sleep(15)
    return t_c_f, t_c_i


def start_flowcept(exp_conf, job_hosts, rep_dir, varying_param_key):
    cluster_utils = BaseClusterUtils.get_instance()
    should_start_mongo = exp_conf.static_params.start_mongo
    flowcept_base_settings_path = exp_conf["static_params"][
        "flowcept_base_settings_path"
    ]
    dask_scheduler_setup_path = exp_conf["static_params"]["dask_scheduler_setup_path"]
    preload_scheduler_cmd = f"--preload {dask_scheduler_setup_path}"
    flowcept_settings = OmegaConf.load(Path(flowcept_base_settings_path))

    # Sometimes, like on Summit, the 0th node is not a compute node.
    # Here we are setting the 1st node to be the DB host for Redis and Mongo
    # (if we want to start mongo in this script)
    db_host = list(job_hosts.keys())[1]
    print(f"DB Host: {db_host}")
    job_id = cluster_utils.get_this_job_id()
    update_flowcept_settings(
        exp_conf,
        flowcept_settings,
        db_host,
        should_start_mongo,
        rep_dir,
        varying_param_key,
        job_id,
    )
    kill_dbs(db_host, should_start_mongo)
    redis_start_command = exp_conf.static_params.redis_start_command
    start_redis(db_host, redis_start_command)
    if should_start_mongo:
        mongo_start_cmd = exp_conf.static_params.mongo_start_command
        start_mongo(db_host, mongo_start_cmd, rep_dir)

    from flowcept import FlowceptConsumerAPI

    consumer = FlowceptConsumerAPI()
    consumer.start()
    return consumer, flowcept_settings, preload_scheduler_cmd


def main(
    exp_conf: DictConfig,
    varying_param_key: str,
    my_job_id,
    rep_no: int,
):
    cluster_utils = BaseClusterUtils.get_instance()
    proj_dir = exp_conf.static_params.proj_dir
    job_dir = os.path.join(proj_dir, "exps", my_job_id)
    rep_dir = os.path.join(job_dir, str(rep_no))
    os.makedirs(rep_dir, exist_ok=True)
    nnodes = exp_conf.varying_params[varying_param_key].get("nnodes")
    n_gpus_per_node = exp_conf.static_params.get("n_gpus_per_node")
    gpu_type = exp_conf.static_params.get("gpu_type")
    scheduler_file = os.path.join(rep_dir, "scheduler_info.json")

    with_flowcept = exp_conf.varying_params[varying_param_key].get(
        "with_flowcept", False
    )
    with_flowcept_arg = "--with-flowcept" if with_flowcept else ""
    # db_host = None

    os.environ["LC_ALL"] = "C"
    os.environ["LANG"] = "C"
    python_env = run_cmd_check_output("which python")
    print(f"Using python: {python_env}")  # TODO: save this in the workflow?

    job_hosts = cluster_utils.get_job_hosts()
    host_allocs = {}  # TODO: not sure if we are going to need this
    preload_scheduler_cmd = ""

    t0 = time()
    # run_cmd(f"jskill all ") TODO?
    printed_sleep(2)

    consumer = None
    flowcept_settings = None
    if with_flowcept:
        consumer, flowcept_settings, preload_scheduler_cmd = start_flowcept(
            exp_conf, job_hosts, rep_dir, varying_param_key
        )

    if not start_scheduler(preload_scheduler_cmd, rep_dir, scheduler_file):
        return -1

    printed_sleep(3)
    start_workers_with_gpu(nnodes, n_gpus_per_node, gpu_type, rep_dir, scheduler_file)

    t_c_f, t_c_i = start_client(exp_conf, varying_param_key, with_flowcept_arg)
    print("Workflow done!")
    if consumer is not None:
        print("Now going to gracefully stop everything")
        consumer.stop()

    with open(f"{rep_dir}/workflow_result.json", "r") as infile:
        wf_result = json.load(infile)

    t1 = time()
    job_output = cluster_utils.generate_job_output(
        cluster_utils,
        exp_conf,
        host_allocs,
        job_hosts,
        job_dir,
        my_job_id,
        proj_dir,
        python_env,
        rep_dir,
        rep_no,
        t0,
        t1,
        t_c_f,
        t_c_i,
        varying_param_key,
        wf_result,
        with_flowcept,
        flowcept_settings,
    )

    if with_flowcept:
        test_data_and_persist(rep_dir, wf_result, job_output)

    print("All done. Going to kill other processes")
    # run_cmd("jskill all") # TODO?


if __name__ == "__main__":
    args = parse_args()

    exp_conf = OmegaConf.load(Path(args.conf))

    nreps = exp_conf.varying_params[args.varying_param_key]["nreps"]
    for rep_no in range(nreps):
        main(
            exp_conf=exp_conf,
            varying_param_key=args.varying_param_key,
            my_job_id=args.my_job_id,
            rep_no=rep_no,
        )

    proj_dir = exp_conf.static_params.proj_dir
    job_dir = os.path.join(proj_dir, "exps", args.my_job_id)
    os.makedirs(job_dir, exist_ok=True)
    with open(os.path.join(job_dir, "SUCCESS"), "w") as f:
        f.write(datetime.utcnow().strftime("%Y-%m-%d %H-%M-%S.%f")[:-3])

    BaseClusterUtils.get_instance().kill_this_job()
    sys.exit(0)
