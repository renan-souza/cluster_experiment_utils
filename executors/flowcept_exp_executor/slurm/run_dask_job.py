import sys
import argparse
import os
import re
import subprocess
from dataclasses import dataclass
from pathlib import Path
from time import time, sleep
import json
import yaml
from datetime import datetime

from omegaconf import OmegaConf, DictConfig
from pymongo import MongoClient

from experiment_utils.cluster_utils.base_cluster_utils import BaseClusterUtils
from experiment_utils.cluster_utils.lsf_utils import LsfUtils
from experiment_utils.flowcept_utils import (
    update_flowcept_settings,
    kill_dbs,
    start_redis,
    start_mongo,
)
from experiment_utils.utils import run_cmd_check_output, run_cmd, printed_sleep


@dataclass
class Args:
    my_job_id: str = None
    conf: str = None  # Path
    varying_param_key: str = None


def parse_args():
    parser = argparse.ArgumentParser(description="Submit Dask Job.")

    parser.add_argument(
        "--my-job-id", metavar="i", required=True, help="job id generated by our script"
    )

    parser.add_argument("--conf", help="Yaml configuration file", required=True)

    parser.add_argument("--varying_param_key", required=True)

    if len(sys.argv) == 1:
        parser.print_help()

    ags = parser.parse_args()
    return Args(
        my_job_id=ags.my_job_id, conf=ags.conf, varying_param_key=ags.varying_param_key
    )


def main(conf_data: DictConfig, varying_param_key: str, my_job_id, rep_no: int):
    cluster_scheduler_type = "lsf"
    cluster_utils: BaseClusterUtils = None
    if cluster_scheduler_type == "lsf":
        cluster_utils = LsfUtils()
    elif cluster_scheduler_type == "slurm":
        # cluster_utils = SlurmUtils()
        pass
    else:
        raise Exception("Sorry, next time.")

    proj_dir = conf_data.static_params.proj_dir
    job_dir = os.path.join(proj_dir, "exps", my_job_id)
    rep_dir = os.path.join(job_dir, str(rep_no))
    os.makedirs(rep_dir, exist_ok=True)

    container_images_dir = conf_data.static_params.container_images_dir
    redis_image = os.path.join(container_images_dir, "redis.sif")
    should_start_mongo = conf_data.static_params.start_mongo

    if should_start_mongo:
        mongo_image = os.path.join(container_images_dir, "mongo.sif")

    nnodes = conf_data.varying_params[varying_param_key].get("nnodes")
    with_flowcept = conf_data.varying_params[varying_param_key].get(
        "with_flowcept", False
    )

    # with_flowcept_arg = "--with-flowcept" if with_flowcept else ""
    # db_host = None
    # scheduler_file = os.path.join(rep_dir, "scheduler_info.json")

    os.environ["LC_ALL"] = "C"
    os.environ["LANG"] = "C"
    python_env = run_cmd_check_output("which python")
    print(f"Using python: {python_env}")  # TODO: save this in the workflow?

    host_counts = cluster_utils.get_job_hosts()  # TODO improve var name
    host_allocs = {}
    preload_scheduler_cmd = ""

    t0 = time()
    # run_cmd(f"jskill all ") TODO?
    printed_sleep(2)

    consumer = None
    if with_flowcept:
        flowcept_base_settings_path = conf_data["static_params"][
            "flowcept_base_settings_path"
        ]
        dask_scheduler_setup_path = conf_data["static_params"][
            "dask_scheduler_setup_path"
        ]
        preload_scheduler_cmd = f"--preload {dask_scheduler_setup_path}"

        with open(flowcept_base_settings_path) as f:
            flowcept_settings = yaml.safe_load(f)

        # On Summit, the first node, ie the [0] in host_counts below, is
        # a batch node. We're forcing here the db to run on a compute node.
        db_host = list(host_counts.keys())[1]
        print(f"DB Host: {db_host}")

        job_id = cluster_utils.get_this_job_id()

        update_flowcept_settings(
            conf_data,
            flowcept_settings,
            db_host,
            should_start_mongo,
            rep_dir,
            varying_param_key,
            job_id,
        )

        kill_dbs(db_host, should_start_mongo)
        start_redis(db_host, redis_image)
        if should_start_mongo:
            start_mongo(db_host, mongo_image, rep_dir)

        from flowcept import FlowceptConsumerAPI

        consumer = FlowceptConsumerAPI()
        consumer.start()


if __name__ == "__main__":
    args = parse_args()

    exp_conf = OmegaConf.load(Path(args.conf))

    nreps = exp_conf.varying_params[args.varying_param_key]["nreps"]

    for rep_no in range(nreps):
        main(
            conf_data=exp_conf,
            varying_param_key=args.varying_param_key,
            my_job_id=args.my_job_id,
            rep_no=rep_no,
        )

    proj_dir = exp_conf.static_params.proj_dir
    job_dir = os.path.join(proj_dir, "exps", args.my_job_id)
    os.makedirs(job_dir, exist_ok=True)
    with open(os.path.join(job_dir, "FINISHED"), "w") as f:
        f.write(datetime.utcnow().strftime("%Y-%m-%d %H-%M-%S.%f")[:-3])

    # kill_this_lsf_job() #TODO
    sys.exit(0)
