project:
  debug: true
  json_serializer: default # or complex. If "complex", FlowCept will deal with complex python dicts that may contain JSON unserializable values
  replace_non_json_serializable: true
  performance_logging: false
  register_workflow: false
  enrich_messages: false
  # telemetry_capture:
  #   cpu: false
  #   #gpu: "${oc.env:GPU_CAPTURE,[used]}"
  #   gpu: "None"
  #   per_cpu: false
  #   process_info: false
  #   mem: false
  #   disk: false
  #   network: false
  #   machine_info: false

log:
  log_path: $[log_path]
  log_file_level: $[flowcept_file_log_level]
  log_stream_level: $[flowcept_stdout_log_level]

experiment:
  user: $[user]
  campaign_id: $[campaign_id]

main_redis:
  host: $[db_host]
  port: 6379
  instances: $[redis_instances]
  channel: interception
  buffer_size: 10000
  insertion_buffer_time_secs: 30
 
mongodb:
  host: login13
  port: 27017
  db: flowcept
  collection: tasks
  insertion_buffer_time_secs: 30
  max_buffer_size: 15000
  min_buffer_size: 10000
  remove_empty_fields: false
  create_collection_index: false

sys_metadata:
  environment_id: $[environment_id]

web_server:
  host: 0.0.0.0
  port: 5000

extra_metadata:
  job_id: $[job_id]

analytics:
  sort_orders:
    generated.loss: minimum_first
    generated.accuracy: maximum_first


adapters:
  # For each key below, you can have multiple instances. Like mlflow1, mlflow2; zambeze1, zambeze2. Use an empty dict, [], if you won't use any adapter.

  dask:
    kind: dask
    redis_host: $[db_host]   # dask interceptor also uses dask as KV. This is a different usage from the MQ's Redis.
    redis_port: 6379
    worker_should_get_input: true
    scheduler_should_get_input: true
    worker_should_get_output: true
    scheduler_create_timestamps: false
    worker_create_timestamps: false
