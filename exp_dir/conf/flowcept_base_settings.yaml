project:
  debug: true
  json_serializer: default # or complex. If "complex", FlowCept will deal with complex python dicts that may contain JSON unserializable values
  replace_non_json_serializable: true
  performance_logging: false
  register_workflow: true
  enrich_messages: true
  telemetry_capture:
    gpu: true
    cpu: true
    per_cpu: false
    process_info: false
    mem: true
    disk: false
    network: false
    machine_info: true

log:
  log_path: $[log_path]
  log_file_level: $[log_level]
  log_stream_level: $[log_level]

experiment:
  user: $[user]
  campaign_id: $[campaign_id]

main_redis:
  host: localhost
  port: 6379
  channel: interception
  buffer_size: 50
  insertion_buffer_time_secs: 5

mongodb:
  host: login13
  port: 27017
  db: flowcept
  collection: tasks
  insertion_buffer_time_secs: 5
  max_buffer_size: 50
  min_buffer_size: 10
  remove_empty_fields: false
  create_collection_index: true

sys_metadata:
  environment_id: $[environment_id]

web_server:
  host: 0.0.0.0
  port: 5000

extra_metadata:
  job_id: $[job_id]

analytics:
  sort_orders:
    generated.loss: minimum_first
    generated.accuracy: maximum_first


adapters:
  # For each key below, you can have multiple instances. Like mlflow1, mlflow2; zambeze1, zambeze2. Use an empty dict, [], if you won't use any adapter.

  dask:
    kind: dask
    redis_host: $[db_host]
    redis_port: 6379
    worker_should_get_input: true
    scheduler_should_get_input: true
    worker_should_get_output: true
    scheduler_create_timestamps: true
    worker_create_timestamps: true
